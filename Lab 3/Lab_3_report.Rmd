---
title: "Lab_3_report"
author: "Axel Holmberg (axeho681), Wilhelm Hansson (wilha431)"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
```

## 1

### a)

```{r}
data <-read.delim("rainfall.dat", header=FALSE, sep="\n")[,1]

#Prior values
mu_0 <- mean(data)
sigma_0 <- sd(data)
tau_0 <- 5
v_0 <- 10
n <- length(data)
iterations <- 10000 

#Prepping thetas for gibbs sampling
thetas_col <- c(rep(0,iterations))
thetas <- cbind(thetas_col,thetas_col)
thetas[1,] = c(mu_0, sigma_0^2)

#Function for draws for mu from the conditional posterior
mu_draw_cond_post <- function(sigma_2) {
	tau_n_2 <- 1/(n/sigma_2 + 1/tau_0^2)    
	raw <- (n/sigma_2) / (n/sigma_2 + 1/tau_0^2)
	mu_n <- raw*mu_0 + (1-raw) * mu_0
	return(rnorm(1,mu_n,sqrt(tau_n_2)))
}

#Fucntion for random inverse chi squared
randominvchisq <- function(vn,sigman,ndraw) {
	return(vn*sigman/rchisq(ndraw,vn))
}

#Function for draws for sigma from the conditional posterior
sigma_draw_cond_post <- function(mu) {
	v_n <-  n + v_0 #!
	sigma_n <- (v_0*sigma_0^2 + sum((data-mu)^2))/v_n #!
	return(randominvchisq(v_n,sigma_n,1))
}

#Function for draws from the h
gibbs_draw <- function(theta_n) {
	
	mu <- mu_draw_cond_post(theta_n[2])
	sigma_2 <- sigma_draw_cond_post(mu)
	return(c(mu,sigma_2))
}


#Draws for the set number of observations. First is already set above, hence we start at 2
for (i in 2:iterations) {
	thetas[i,] <- gibbs_draw(thetas[i-1,])
}

plot(thetas[,1], type="l", main="Mu")

hist(thetas[,1], main="Mu")
```

As one can see above the values from the Gibbs model varies as it simulates $\mu$ for the joint posterior.


```{r}

hist(thetas[,1], main="Mu")

plot(thetas[,2], type="l", main="Sigma squared")
```

As one can see above the values from the Gibbs model varies as it simulates $\sigma^2$ for the joint posterior.


### b)

![Iterations](Mix_Norm_Model.png)

The plots above shows iteration 1, 3, 5 and 1000 of the Gibbs sampling data augmentation algorithm. As one can see it starts at the same place, but as it iterates it splits into two normal distributions, each describing a part of the data well. The combined value of this can be seen in the dashed red line showing the Mixture normal model.

![Final Fitted](Final_fitted.png)

The plot above shows the final fitted version of the mixture model as well as a normal distribution based on the mean and variance of the original data.

### c)
 
![Comparison](Comparison.png)

The plot above shows the final fitted version of the mixture model as well as a normal distribution created from values from the Gibbs sampler from 1 a).

One can also see that the normal distribution based on the mean and variance of the original data is almost the exact same as the normal distribution created from values from the Gibbs sampler from 1 a).


## 2

### a)



\newpage

## Appendix for code

```{r, code=readLines("Lab_3.R"), echo=TRUE, eval=FALSE}

```
